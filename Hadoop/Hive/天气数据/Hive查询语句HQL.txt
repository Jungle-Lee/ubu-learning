创建数据库:
	CREATE DATABASE IF NOT EXISTS db_china_weather_air;

显示数据库:
	show databases;
	
切换数据库:
	use db_china_weather_air;

建表:
	内部表：
		CREATE TABLE IF NOT EXISTS tb_dim_area_cn (code string,capital string,region string,province string,city string,district string) row format delimited fields terminated by '\t';
	建分区表:
		CREATE TABLE IF NOT EXISTS tb_part_fact_air_cn ( area string, dt string comment '日期',aqi string comment '空气指数',aqi_range string comment 'AQI指数范围',  quality string comment '空气等级',pm25 string  comment'PM2.5细颗粒物',  pm10 string  comment'PM10可吸入颗粒物',  so2 string comment '二氧化硫(So2)',co string comment '一氧化碳(Co)',  no2 string comment '二氧化氮(No2)',  o3 string comment '臭氧(O3)' ) partitioned by (year string , mounth string)  row format delimited fields terminated by '\t';		
		 
	建外部表（要求制定的目录在hdfs上不存在）：
		-- CREATE external TABLE IF NOT EXISTS tb_external_dim_area_cn (code string,capital string,region string,province string,city string,district string) row format delimited fields terminated by '\t' location '/hive/external_table/tb_dim_area_cn';
		CREATE TABLE tb_external_fact_weather_cn ( area string, dt string, weather string, min_temp int, max_temp int, wind_level string, wind_direct string) row format delimited fields terminated by '\t' location '/hive/external_table/tb_external_fact_weather_cn';
加载数据：
	内部表：
		load data local inpath '/home/txt/dim_area_cn.txt' overwrite into table tb_dim_area_cn ;
	分区表:
		load data local inpath '/home/txt/fact_air_cn2014-01.txt' into table tb_part_fact_air_cn partition (year="2014",mounth='01');
		load data local inpath '/home/txt/fact_air_cn2014-02.txt' into table tb_part_fact_air_cn partition (year="2014",mounth='02');
		load data local inpath '/home/txt/fact_air_cn2014-03.txt' into table tb_part_fact_air_cn partition (year="2014",mounth='03');
		load data local inpath '/home/txt/fact_air_cn2014-04.txt' into table tb_part_fact_air_cn partition (year="2014",mounth='04');
		load data local inpath '/home/txt/fact_air_cn2014-05.txt' into table tb_part_fact_air_cn partition (year="2014",mounth='05');
	外部表：
		hadoop fs -mkdir -p  /hive/external_table/tb_external_fact_weather_cn/
		hadoop fs -put /home/txt/fact_weather_cn.txt /hive/external_table/tb_external_fact_weather_cn/fact_weather_cn.txt

		
查询：
	设置显示表头：
		set hive.cli.print.header=true;
	删除表：
		drop table  tb_dim_area_cn;
	清空表：
		truncate table tb_dim_area_cn;
	查询所有：
		select * from tb_dim_area_cn;   
	条件查询：
		select * from tb_dim_area_cn where city='北京市';
	求平均：
		select avg(min_temp) from tb_external_fact_weather_cn limit 10;
	查询总数：
		select count(1) from tb_external_fact_weather_cn;
	等值连接：
		select * from tb_dim_area_cn t1 , tb_external_fact_weather_cn t2 where t1.city=t2.area limit 1;
	左连接：
		select * from tb_dim_area_cn t1 left join  tb_external_fact_weather_cn t2 on t1.citititysityaityity=t2.area where t1.district='昌平区';

创建带集合数据类型的表：
	create table tb_employees( id int , name string , salary float,subordinates array<string>,deduction map<string , float>, address struct<street : string , city:string , province :string , nation:string> ) row format delimited fields terminated by '^' collection items terminated by '<' map keys terminated by '>' lines terminated by '\n' stored as textfile;
	load data local inpath '/home/txt/employees.txt' into table tb_employees ;
	select id ,name , salary , subordinates[0] , deduction['TAX1'] , address.city , address.province  from tb_employees;
	
	

查看表信息：
	describe tb_dim_area_cn;
	describe extended  tb_dim_area_cn;
	describe formatted  tb_dim_area_cn;
	describe tb_dim_area_cn.code;   
	
外部分区表：
	hadoop fs  -mkdir -p /hive/external_part_fact_air_cn/2014/03
	hadoop fs  -mkdir -p /hive/external_part_fact_air_cn/2014/04
	hadoop fs  -mkdir -p /hive/external_part_fact_air_cn/2014/05

	CREATE external TABLE IF NOT EXISTS tb_external_part_fact_air_cn ( area string, dt string comment '日期',aqi string comment '空气指数',aqi_range string comment 'AQI指数范围',  quality string comment '空气等级',pm25 string  comment'PM2.5细颗粒物',  pm10 string  comment'PM10可吸入颗粒物',  so2 string comment '二氧化硫(So2)',co string comment '一氧化碳(Co)',  no2 string comment '二氧化氮(No2)',  o3 string comment '臭氧(O3)' ) partitioned by (year string , mounth string)  row format delimited fields terminated by '\t'  location '/hive/external_part_fact_air_cn';	
	
	
	hadoop fs -put /home/txt/fact_air_cn2014-03.txt /hive/external_part_fact_air_cn/2014/03/fact_air_cn2014-03.txt
	hadoop fs -put /home/txt/fact_air_cn2014-04.txt /hive/external_part_fact_air_cn/2014/04/fact_air_cn2014-04.txt
	hadoop fs -put /home/txt/fact_air_cn2014-05.txt /hive/external_part_fact_air_cn/2014/05/fact_air_cn2014-05.txt


增加、删除、修改表分区：	
	alter table tb_external_part_fact_air_cn add partition (year=2014,mounth=04) location '/hive/external_part_fact_air_cn/2014/04';
	alter table tb_external_part_fact_air_cn add partition (year=2014,mounth=05) location '/hive/external_part_fact_air_cn/2014/05';
	
	alter table tb_external_part_fact_air_cn partition(year=2014,mounth=04) SET LOCATION 'hdfs://myj03:9000/hive/external_part_fact_air_cn/2014/05';
	--(制定路径报错：/hive/external_part_fact_air_cn/2014/05)
	
	ALTER TABLE tb_external_part_fact_air_cn DROP IF EXISTS PARTITION (year=2014,mounth=04);
	ALTER TABLE tb_external_part_fact_air_cn DROP IF EXISTS PARTITION (year=2014,mounth=05);


查看分区：
	show partitions tb_external_part_fact_air_cn;
	
	
	
	create table tb_test(id int  , name string , age int  );   
删除表：
	drop table if exists tb_test;

   
	
修改表

重命名表：
	alter table tb_test rename to tb_test_rename;	
	


修改列信息：
	alter table tb_test change column name name_r string  comment 'update by myj ' after age;	
	alter table tb_test change column name name_r string  comment 'update by myj ' first;	
	
增加列：
	alter table tb_test add columns( phone string comment '手机号码', address string comment '住址' );	

替换列：
	alter table tb_test replace COLUMNS   (id int , name string  );               

删除列：（使用replace代替）
	alter table tb_test drop COLUMN address ;(FAILED: ParseException line 1:25 mismatched input 'column' expecting PARTITION near 'drop' in drop partition statement)
	
修改表属性：
	alter table tb_test set tblproperties ('notes'='add notes');


通过查询语句向表中插入数据：
	CREATE TABLE IF NOT EXISTS tb_dim_area_cn_back (code string,capital string,region string,province string,city string,district string) row format delimited fields terminated by '\t';
	insert overwrite  table tb_dim_area_cn_back  select * from tb_dim_area_cn limit 10;（之前表中的内容会被覆盖）
	insert into  table tb_dim_area_cn_back  select * from tb_dim_area_cn where city='昆明市'; （在原内容中追加）

单个查询语句创建表并加载数据：	
	create table tb_dim_area_cn_back_v2 as select * from tb_dim_area_cn_back;
	
将表中的文件拷贝到本地：	
	hadoop fs -get /user/hive/warehouse/db_china_weather_air.db/tb_dim_area_cn/dim_area_cn.txt  /home/txt/get/dim_area_cn.txt
	insert overwrite local directory '/home/txt/get/dim_area_cn2.txt'  select * from tb_dim_area_cn;

函数：104页
表生成函数：
	select explode( array(123,456)) from tb_dim_area_cn limit 1;
	select parse_url_tuple('http://myj03:50070/explorer.html?data=123' , 'HOST' ,'PATH' , 'QUERY') as (host , path , query) from tb_dim_area_cn limit 1;
	
嵌套查询：
	from (select * from tb_dim_area_cn limit 10) e select e.* where code in ('110000' , '110001');

case when：
	select id , name , age , case when height < 165 then 'low' when height < 175 then  'middle'  else 'height' end  as type from tb_staff;

视图：(139页)
	
	 from (select * from tb_dim_area_cn where province='北京市') tb_a select *   where tb_a.district='北京市' ;   

创建视图：
	create view tb_dim_area_cn_capital_b as select * from tb_dim_area_cn where capital='B' ;

删除视图
	drop view tb_dim_area_cn_capital_b;
	
查询视图：
	select * from tb_dim_area_cn_capital_b where district='北京市';


索引：(144页)
	select * from tb_external_fact_weather_cn where area='广州市' and dt='2014-12-25' ;
创建索引：
	create index area_index on table tb_external_fact_weather_cn(area)   as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' with deferred rebuild IN TABLE area_index_table;
索引重建：
	alter index area_index on tb_external_fact_weather_cn rebuild;		
	
	create external table tb_dynamictable(cols map<string , string>) row format delimited fields terminated by ',' collection items terminated by '>' map keys terminated by '^' stored as textfile;
	hadoop fs -put /home/opt/hive/hivetable/tb_dynamictable.txt /user/hive/warehouse/testdb.db/tb_dynamictable/
	select cols["type"] from tb_dynamictable where cols["type"]="request"; 
	
	create view tb_dynamictable_view(time,type,state ) as  select cols["time"] , cols["type"] , cols["state"] from tb_dynamictable;
	select * from tb_dynamictable_view;


调优
	explain expended
	explain extended  SELECT * FROM tb_part_fact_air_cn WHERE year='2014' and mounth='01' LIMIT 10;


	